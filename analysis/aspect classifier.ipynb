{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import copy\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from absa import train_reviews_path, test_reviews_path, TEST_APPENDIX, word2vec_model_path, \\\n",
    "                parsed_reviews_dump_path, PROGRESSBAR_COLUMNS_NUM, images_path\n",
    "from absa.preprocess.spell_check import spell_check\n",
    "from absa.preprocess.dependency import dep_parse_reviews\n",
    "from absa.models.level.sentence.aspect.classifier import AspectClassifier as SentenceAspectClassifier\n",
    "from absa.models.level.opinion.aspect.classifier import AspectClassifier as OpinionAspectClassifier\n",
    "from absa.utils.embedding import Embeddings\n",
    "from absa.utils.dump import load_dump\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "th.cuda.manual_seed(seed)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Embeddings.vocabulary\n",
    "embeddings_matrix = Embeddings.embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Upload from dump: /home/dmitry/Projects/absa/dumps/data/dep_parsed_sentence\n",
      "INFO:root:Upload from dump: /home/dmitry/Projects/absa/dumps/data/dep_parsed_sentence.test\n"
     ]
    }
   ],
   "source": [
    "train_texts = load_dump(pathway=parsed_reviews_dump_path)\n",
    "test_texts = load_dump(pathway=parsed_reviews_dump_path + TEST_APPENDIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_NAME = 'F1'\n",
    "PARAMETER_DECIMAL_LEN = 5\n",
    "SCORE_DECIMAL_LEN = 3\n",
    "\n",
    "\n",
    "def display_score(parameter_values: List,\n",
    "                  train_values: np.array,\n",
    "                  val_values: np.array,\n",
    "                  parameter_name='Epoch',\n",
    "                  score_name=SCORE_NAME) -> float:\n",
    "\n",
    "    max_param, max_acc = [(parameter_values[index], val)\n",
    "                          for index, val in enumerate(val_values) if val == max(val_values)][0]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(left=min(parameter_values), right=max(parameter_values))\n",
    "\n",
    "    # train\n",
    "    plt.plot(parameter_values, train_values, color='blue')\n",
    "    # validation\n",
    "    plt.plot(parameter_values,\n",
    "             val_values,\n",
    "             color='#EE6B24')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.title(f'Dependence of {score_name} from {parameter_name}')\n",
    "    plt.xlabel(f'{parameter_name}')\n",
    "    plt.ylabel(score_name.capitalize())\n",
    "    if isinstance(max_param, (int, )):\n",
    "        plt.legend([\n",
    "            f'Maximal {score_name}={max_acc:.{SCORE_DECIMAL_LEN}} when {parameter_name}={max_param}'\n",
    "        ])\n",
    "    else:\n",
    "        plt.legend([\n",
    "            f'Maximal {score_name}={max_acc:.{SCORE_DECIMAL_LEN}}' +\n",
    "            f' when {parameter_name}={max_param:.{PARAMETER_DECIMAL_LEN}f}'\n",
    "        ])\n",
    "    plt.legend(['Train', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_number = 5\n",
    "kf = KFold(n_splits=splits_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-Level Aspect Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "cv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 50/50 [00:37<00:00,  1.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 50/50 [00:36<00:00,  1.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 50/50 [00:38<00:00,  1.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 50/50 [00:37<00:00,  1.32it/s]\n",
      "  4%|██▌                                                             | 2/50 [00:01<00:32,  1.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/Projects/absa/absa/models/level/sentence/aspect/classifier.py:243: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision = correct / total_predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 50/50 [00:35<00:00,  1.41it/s]\n",
      "  6%|███▊                                                            | 3/50 [00:02<00:33,  1.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/Projects/absa/absa/models/level/sentence/aspect/classifier.py:245: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1 = 2 * (precision * recall) / (precision + recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 50/50 [00:38<00:00,  1.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 50/50 [00:56<00:00,  1.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=AspectClassifier(batch_size=100,\n",
       "                                        nn_params=<frozendict {'layers_dim': array([40])}>,\n",
       "                                        num_epoch=50,\n",
       "                                        optimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "                                        optimizer_params=<frozendict {'lr': 0.01}>),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'batch_size': [100],\n",
       "                         'nn_params': [{'layers_dim': array([40, 20])},\n",
       "                                       {'layers_dim': array([44, 22])}]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'batch_size': [100,],\n",
    "    'nn_params': [{'layers_dim': np.array((x*2, x))} for x in range(20, 24, 2)]\n",
    "}\n",
    "clf = GridSearchCV(SentenceAspectClassifier(), \n",
    "                   parameters, \n",
    "                   cv=cv)\n",
    "clf.fit(train_texts, \n",
    "        vocabulary=vocabulary, \n",
    "        embeddings=embeddings_matrix,\n",
    "        save_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([39.71655027, 38.20819012]),\n",
       " 'std_fit_time': array([1.85015788, 1.43609684]),\n",
       " 'mean_score_time': array([0.55217187, 0.50693202]),\n",
       " 'std_score_time': array([0.05934784, 0.06081869]),\n",
       " 'param_batch_size': masked_array(data=[100, 100],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_nn_params': masked_array(data=[{'layers_dim': array([40, 20])},\n",
       "                    {'layers_dim': array([44, 22])}],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'batch_size': 100, 'nn_params': {'layers_dim': array([40, 20])}},\n",
       "  {'batch_size': 100, 'nn_params': {'layers_dim': array([44, 22])}}],\n",
       " 'split0_test_score': array([0.71224018, 0.7271854 ]),\n",
       " 'split1_test_score': array([0.73966579, 0.71981132]),\n",
       " 'split2_test_score': array([0.70990056, 0.71992819]),\n",
       " 'mean_test_score': array([0.72060218, 0.7223083 ]),\n",
       " 'std_test_score': array([0.0135138 , 0.00344896]),\n",
       " 'rank_test_score': array([2, 1], dtype=int32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:58<00:00,  1.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:57<00:00,  1.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:58<00:00,  1.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:56<00:00,  1.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:57<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_train_f1_history = np.zeros(shape=(num_epoch,))\n",
    "sentence_val_f1_history = np.zeros(shape=(num_epoch,))\n",
    "threshold = None\n",
    "\n",
    "for train_index, val_index in kf.split(train_texts):\n",
    "    classifier = SentenceAspectClassifier(vocabulary=vocabulary, emb_matrix=embeddings_matrix)\n",
    "    t, v = classifier.fit(\n",
    "        train_texts=[train_texts[x] for x in train_index], \n",
    "        val_texts=[train_texts[x] for x in val_index],\n",
    "        num_epoch=num_epoch)\n",
    "    sentence_train_f1_history += t\n",
    "    sentence_val_f1_history += v\n",
    "    if threshold is not None:\n",
    "        threshold += classifier.threshold\n",
    "    else:\n",
    "        threshold = classifier.threshold\n",
    "sentence_train_f1_history /= splits_number\n",
    "sentence_val_f1_history /= splits_number\n",
    "threshold /= splits_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'sentence_train_f1_history' (ndarray)\n",
      "Stored 'sentence_val_f1_history' (ndarray)\n",
      "Stored 'threshold' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store sentence_train_f1_history sentence_val_f1_history threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_score(parameter_values=[x for x in range(num_epoch)],\n",
    "              train_values=sentence_train_f1_history,\n",
    "              val_values=sentence_val_f1_history,\n",
    "              score_name='F1 score')\n",
    "plt.savefig(os.path.join(images_path, 'sentence_level_aspect_classifier_f1_vs_epoch.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentenceAspectClassifier(vocabulary=vocabulary,\n",
    "                                      emb_matrix=embeddings_matrix)\n",
    "_ = classifier.fit(train_texts=train_texts,\n",
    "                   init_threshold=threshold,\n",
    "                   fixed_threshold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_pred = copy.deepcopy(test_texts)\n",
    "for text in test_texts_pred:\n",
    "    text.reset_opinions()\n",
    "\n",
    "test_texts_pred = classifier.predict(test_texts_pred)\n",
    "score = classifier.score(texts=test_texts, texts_pred=test_texts_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion-Level Aspect Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "opinion_train_f1_history = np.zeros(shape=(num_epoch), dtype=np.float)\n",
    "opinion_val_f1_history = np.zeros(shape=(num_epoch), dtype=np.float)\n",
    "\n",
    "for train_index, val_index in kf.split(train_texts):\n",
    "    classifier = OpinionAspectClassifier(vocabulary=vocabulary,\n",
    "                                         emb_matrix=embeddings_matrix)\n",
    "    t, v = classifier.fit(\n",
    "        train_texts=[train_texts[x] for x in train_index], \n",
    "        val_texts=[train_texts[x] for x in val_index],\n",
    "        num_epoch=num_epoch)\n",
    "    opinion_train_f1_history += t\n",
    "    opinion_val_f1_history += v\n",
    "opinion_train_f1_history /= splits_number\n",
    "opinion_val_f1_history /= splits_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store opinion_train_f1_history opinion_val_f1_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_score(parameter_values=[x for x in range(num_epoch)],\n",
    "              train_values=opinion_train_f1_history,\n",
    "              val_values=opinion_val_f1_history,\n",
    "              score_name='F1 score')\n",
    "plt.savefig(os.path.join(images_path, 'target_level_aspect_classifier_f1_vs_epoch.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = OpinionAspectClassifier(vocabulary=vocabulary,\n",
    "                                     emb_matrix=embeddings_matrix)\n",
    "_ = classifier.fit(train_texts=train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_pred = classifier.predict(test_texts_pred)\n",
    "score = classifier.score(texts=test_texts, texts_pred=test_texts_pred)\n",
    "print(f'{score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "414.653px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
